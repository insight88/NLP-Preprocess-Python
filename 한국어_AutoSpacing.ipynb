{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#출처 : https://github.com/haven-jeon/TrainKoSpacing\n",
    "import argparse\n",
    "import bz2\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "import mxnet.autograd as autograd\n",
    "import numpy as np\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.embedding_maker import (encoding_and_padding, load_embedding, load_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class korean_autospacing_base(gluon.HybridBlock):\n",
    "    def __init__(self, n_hidden, vocab_size, embed_dim, max_seq_length,\n",
    "                 **kwargs):\n",
    "        super(korean_autospacing_base, self).__init__(**kwargs)\n",
    "        \n",
    "        self.in_seq_len = max_seq_length  # 입력 시퀀스 길이\n",
    "        self.out_seq_len = max_seq_length  # 출력 시퀀스 길이\n",
    "        self.n_hidden = n_hidden  # GRU의 hidden 개수\n",
    "        self.vocab_size = vocab_size  # 고유문자개수\n",
    "        self.max_seq_length = max_seq_length  # max_seq_length\n",
    "        self.embed_dim = embed_dim  # 임베딩 차원수\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim)\n",
    "            self.conv_unigram = nn.Conv2D(channels=128, kernel_size=(1, self.embed_dim))\n",
    "            self.conv_bigram = nn.Conv2D(channels=256,\n",
    "                                         kernel_size=(2, self.embed_dim),\n",
    "                                         padding=(1, 0))\n",
    "            self.conv_trigram = nn.Conv2D(channels=128,\n",
    "                                          kernel_size=(3, self.embed_dim),\n",
    "                                          padding=(1, 0))\n",
    "            self.conv_forthgram = nn.Conv2D(channels=64,\n",
    "                                            kernel_size=(4, self.embed_dim),\n",
    "                                            padding=(2, 0))\n",
    "            self.conv_fifthgram = nn.Conv2D(channels=32,\n",
    "                                            kernel_size=(5, self.embed_dim),\n",
    "                                            padding=(2, 0))\n",
    "\n",
    "            self.bi_gru = rnn.GRU(hidden_size=self.n_hidden, layout='NTC', bidirectional=True)\n",
    "            self.dense_sh = nn.Dense(100, activation='relu', flatten=False)\n",
    "            self.dense = nn.Dense(1, activation='sigmoid', flatten=False)\n",
    "\n",
    "    def hybrid_forward(self, F, inputs):\n",
    "        embed = self.embedding(inputs)\n",
    "        embed = F.expand_dims(embed, axis=1)\n",
    "        unigram = self.conv_unigram(embed)\n",
    "        bigram = self.conv_bigram(embed)\n",
    "        trigram = self.conv_trigram(embed)\n",
    "        forthgram = self.conv_forthgram(embed)\n",
    "        fifthgram = self.conv_fifthgram(embed)\n",
    "\n",
    "        grams = F.concat(unigram,\n",
    "                         F.slice_axis(bigram, axis=2, begin=0, end=self.max_seq_length),\n",
    "                         trigram,\n",
    "                         F.slice_axis(forthgram, axis=2, begin=0, end=self.max_seq_length),\n",
    "                         F.slice_axis(fifthgram, axis=2, begin=0, end=self.max_seq_length),\n",
    "                         dim=1)\n",
    "\n",
    "        grams = F.transpose(grams, (0, 2, 3, 1))\n",
    "        grams = F.reshape(grams, (-1, self.max_seq_length, -3))\n",
    "        grams = self.bi_gru(grams)\n",
    "        fc1 = self.dense_sh(grams)\n",
    "        return (self.dense(fc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_spacing:\n",
    "    def __init__(self, model, w2idx, max_seq_len):\n",
    "        self.model = model\n",
    "        self.w2idx = w2idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pattern = re.compile(r'\\s+')\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_spaced_sent(self, raw_sent):\n",
    "        raw_sent_ = \"«\" + raw_sent + \"»\"\n",
    "        raw_sent_ = raw_sent_.replace(' ', '^')\n",
    "        sents_in = [\n",
    "            raw_sent_,\n",
    "        ]\n",
    "        mat_in = encoding_and_padding(word2idx_dic=self.w2idx,\n",
    "                                      sequences=sents_in,\n",
    "                                      maxlen=self.max_seq_len,\n",
    "                                      padding='post',\n",
    "                                      truncating='post')\n",
    "        mat_in = mx.nd.array(mat_in, ctx=mx.cpu(0))\n",
    "        results = self.model(mat_in)\n",
    "        mat_set = results[0, ]\n",
    "        preds = np.array(\n",
    "            ['1' if i > 0.5 else '0' for i in mat_set[:len(raw_sent_)]])\n",
    "        return self.make_pred_sents(raw_sent_, preds)\n",
    "\n",
    "    def make_pred_sents(self, x_sents, y_pred):\n",
    "        res_sent = []\n",
    "        for i, j in zip(x_sents, y_pred):\n",
    "            if j == '1':\n",
    "                res_sent.append(i)\n",
    "                res_sent.append(' ')\n",
    "            else:\n",
    "                res_sent.append(i)\n",
    "        subs = re.sub(self.pattern, ' ', ''.join(res_sent).replace('^', ' '))\n",
    "        subs = subs.replace('«', '')\n",
    "        subs = subs.replace('»', '')\n",
    "        return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'model/w2idx.dic'\n",
    "embedding_file = 'model/kospacing_wv.np'\n",
    "model_params = 'model/kospacing.params'\n",
    "model_type = 'kospacing'\n",
    "n_hidden = 200\n",
    "max_seq_len = 200\n",
    "w2idx, idx2w = load_vocab(vocab_file)\n",
    "weights = load_embedding(embedding_file)\n",
    "vocab_size = weights.shape[0]\n",
    "embed_dim = weights.shape[1]\n",
    "model = korean_autospacing_base(n_hidden=n_hidden,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    embed_dim=embed_dim,\n",
    "                                    max_seq_length=max_seq_len)\n",
    "model.load_parameters(model_params, ctx=mx.cpu(0))\n",
    "predictor = pred_spacing(model, w2idx, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자열양쪽끝밑단어사이의모든공백을제거하고싶습니다.\n",
      "문자열 양쪽 끝 밑 단어 사이의 모든 공백을 제거하고 싶습니다. \n"
     ]
    }
   ],
   "source": [
    "sent = input()\n",
    "sent.replace(\" \", \"\")\n",
    "print(predictor.get_spaced_sent(sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
